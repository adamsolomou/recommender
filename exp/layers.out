Sender: LSF System <lsfadmin@lo-s4-019>
Subject: Job 2281458: <python exp_autoencoder.py> in cluster <leonhard> Done

Job <python exp_autoencoder.py> was submitted from host <lo-login-01> by user <saioanni> in cluster <leonhard> at Mon Jun 24 11:03:15 2019
Job was executed on host(s) <lo-s4-019>, in queue <gpu.24h>, as user <saioanni> in cluster <leonhard> at Mon Jun 24 11:03:23 2019
</cluster/home/saioanni> was used as the home directory.
</cluster/home/saioanni/CIL/recommender/src> was used as the working directory.
Started at Mon Jun 24 11:03:23 2019
Terminated at Mon Jun 24 13:23:56 2019
Results reported at Mon Jun 24 13:23:56 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python exp_autoencoder.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   14992.00 sec.
    Max Memory :                                 1060 MB
    Average Memory :                             960.39 MB
    Total Requested Memory :                     64192.00 MB
    Delta Memory :                               63132.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                35
    Run time :                                   8455 sec.
    Turnaround time :                            8441 sec.

The output (if any) follows:

2019-06-24 11:03:32.842146: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using TensorFlow backend.
Found GPU at: 
layer: 512
1.3660429329053867
layer: 256
1.2736565640716895
layer: 128
1.1657804265112173
layer: 64
1.069664041071284
layer: 32
1.026102629891478
layer: 16
0.9998904087676466
layer: 15
0.9973426892845749
layer: 14
0.9969015998314684
layer: 13
0.995114662141238
layer: 12
0.9929266703459173
layer: 11
0.9939285910241233
layer: 10
0.9909947538109123
layer: 9
0.992564175709322
layer: 8
0.9920386320583611
layer: 7
0.9891244958076582
layer: 6
0.9881070652913942
layer: 5
0.988725857143945
layer: 4
0.991132409436504
layer: 3
0.992020687427762
layer: 2
0.9943595779964498
